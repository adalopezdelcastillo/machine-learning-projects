{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ffcfafc-a17e-45a4-b9c7-1ea866bc2df3",
   "metadata": {},
   "source": [
    "## Q-learning en una màquina expenedora intel·ligent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b116246d-b20f-402f-bcce-6dbd4c5fa081",
   "metadata": {},
   "source": [
    "#### Ada López del Castillo Avilés\n",
    "#### NIU: 1605347"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba2e49b-f038-4cf9-9708-1b51e544ef79",
   "metadata": {},
   "source": [
    "### Exercici 1: Afegir un estat addicional (L: estoc baix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e507ab7a-0ea2-4947-84da-d3e860ad730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76d1400b-b729-46ca-a192-42d148db7d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table:\n",
      "C: {1: 9.409691531250001, 2: 5.000945543671875, 3: 5.459486189062501}\n",
      "NC: {1: -1.5, 2: 0, 3: 7.779878985937501}\n",
      "L: {1: 5.938257882958073, 2: 13.841464220608106, 3: 19.38872708173352}\n"
     ]
    }
   ],
   "source": [
    "# Paràmetres\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "epsilon = 0.3\n",
    "n_iter = 100\n",
    "\n",
    "# Estats i accions \n",
    "states = [\"C\", \"NC\", \"L\"] # C: Clients, NC: No clients, L: Low stock\n",
    "actions = [1, 2, 3]  # 1: Promo forta, 2: Promo suau, 3: No fer res\n",
    "\n",
    "#Inicialització de la taula amb valors 0\n",
    "Q = {s: {a: 0 for a in actions} for s in states}\n",
    "\n",
    "#Funció recompensa\n",
    "#Afegim les recompenses del Low Stock, imposant que només sigui positiva l'acció 3\n",
    "def get_reward(state, action):\n",
    "    if state == \"C\":\n",
    "        return {1: 6, 2: 3, 3: 0}[action]\n",
    "    elif state == \"NC\":\n",
    "        return {1: -3, 2: -1, 3: 1}[action]\n",
    "    elif state == \"L\":\n",
    "        return {1: -5, 2: -3, 3: 2}[action]\n",
    "#Fem un comptador per controlar el número de promocions que fa\n",
    "comptador = 0\n",
    "\n",
    "#Funció transició amb probabilitats\n",
    "def next_state(state, action, comptador):\n",
    "\n",
    "    #Afegim que si fem acció 1 o 2 (és a dir, qualsevol promo), es suma 1 al comptador.\n",
    "    if action in [1,2]:\n",
    "        comptador += 1\n",
    "    \n",
    "    #Imposem que si s'arriba a més de 5 promocions, passem directament al estat L, ja que per excès de promocions ens hem quedats sense productes.\n",
    "    if comptador > 5:\n",
    "        return \"L\", comptador \n",
    "        \n",
    "    if state == \"C\":\n",
    "        return (np.random.choice([\"NC\", \"C\"], p=[0.7, 0.3]), comptador) if action in [1, 2] else (np.random.choice([\"C\", \"NC\"], p=[0.4, 0.6]), comptador)\n",
    "    \n",
    "    elif state == \"NC\":\n",
    "        return np.random.choice([\"C\", \"NC\"], p=[0.5, 0.5]), comptador\n",
    "\n",
    "    #Afegim llavors que si estem en l'estat L, per excès de promocions, el comptador torna a 0 perquè hi ha hagut reposició (ho suposem)\n",
    "    #A més, li diem que només passi a estar Low Stock o NoClients, amb prob 0.4 i 0.6 respectivament. \n",
    "    elif state == \"L\":\n",
    "        comptador = 0 \n",
    "        return np.random.choice([\"L\", \"NC\"], p=[0.4, 0.6]), comptador        \n",
    "\n",
    "#Notem que hem hagut d'afegir 'comptador' a cadascun dels returns, per guardar la informació i així saber quan arribem al màxim de promos.\n",
    "\n",
    "#Funció acció aleatoria amb e-greedy\n",
    "def choose_action(state):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(actions)\n",
    "    qvals = Q[state]\n",
    "    max_val = max(qvals.values())\n",
    "    best_actions = [a for a in actions if qvals[a] == max_val]\n",
    "    return random.choice(best_actions)\n",
    "\n",
    "state = \"C\"\n",
    "\n",
    "#Bucle principal de Q-learning\n",
    "for i in range(n_iter):\n",
    "    action = choose_action(state)\n",
    "    reward = get_reward(state, action)\n",
    "    next_s, comptador = next_state(state, action, comptador)\n",
    "    Q[state][action] += alpha * (reward + gamma * max(Q[next_s].values()) - Q[state][action])\n",
    "    state = next_s\n",
    "#S'ha hagut de posar el comptador per tal d'evitar errors\n",
    "\n",
    "# Resultats\n",
    "print(\"Q-table:\")\n",
    "for s in Q:\n",
    "    print(f\"{s}: {Q[s]}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7323920e-0668-4ebd-99ad-23edbbf80fc0",
   "metadata": {},
   "source": [
    "### Exercici 2: Accions estocàstiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79cbeba2-daea-498d-88de-341b17c4b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#En aquest apartat només calia canviar la funció get_reward per fer les accions estocàstiques, la resta del codi es manté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf907b74-ba63-4a71-b097-8fb3275bdff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table:\n",
      "C: {1: np.float64(3.0), 2: np.float64(1.9543750000000002), 3: np.float64(0.8794687500000001)}\n",
      "NC: {1: np.float64(-1.5), 2: np.float64(-0.5), 3: np.float64(2.2410312500000003)}\n",
      "L: {1: np.float64(10.063488094382052), 2: np.float64(13.836669907262097), 3: np.float64(19.22077465647504)}\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "epsilon = 0.3\n",
    "n_iter = 100\n",
    "\n",
    "states = [\"C\", \"NC\", \"L\"] \n",
    "actions = [1, 2, 3]  \n",
    "\n",
    "Q = {s: {a: 0 for a in actions} for s in states}\n",
    "\n",
    "#Observem que treiem el return de les recompenses per poder aplicar estoicisitat,\n",
    "#i les guardem amb el nom reward, per a més endavant depenent de la acció que sigui, que passi una cosa o una altra.\n",
    "def get_reward(state, action):\n",
    "    if state == \"C\":\n",
    "        reward = {1: 6, 2: 3, 3: 0}[action]\n",
    "    elif state == \"NC\":\n",
    "        reward = {1: -3, 2: -1, 3: 1}[action]\n",
    "    elif state == \"L\":\n",
    "        reward = {1: -5, 2: -3, 3: 2}[action]\n",
    "    \n",
    "    #Acció 1 falla un 10% i no ven res, per tant pèrdua amb recompensa -1\n",
    "    if action == 1:\n",
    "        return np.random.choice([reward, -1], p=[0.9, 0.1])\n",
    "\n",
    "    #Acció 2 redueix la recompensa en un 20% dels casos, la qual he decidit que sigui una reducció de 2.\n",
    "    if action == 2:\n",
    "        return np.random.choice([reward, reward - 2], p=[0.8, 0.2])\n",
    "    \n",
    "    #Acció 3 en un 5% dels casos falla en no mantenir clients, amb una petita penalització de -1.\n",
    "    if action == 3:\n",
    "        return np.random.choice([reward, -1], p=[0.95, 0.05])\n",
    "\n",
    "comptador = 0\n",
    "\n",
    "def next_state(state, action, comptador):\n",
    "\n",
    "    if action in [1,2]:\n",
    "        comptador += 1\n",
    "    \n",
    "    if comptador > 5:\n",
    "        return \"L\", comptador \n",
    "        \n",
    "    if state == \"C\":\n",
    "        return (np.random.choice([\"NC\", \"C\"], p=[0.7, 0.3]), comptador) if action in [1, 2] else (np.random.choice([\"C\", \"NC\"], p=[0.4, 0.6]), comptador)\n",
    "    \n",
    "    elif state == \"NC\":\n",
    "        return np.random.choice([\"C\", \"NC\"], p=[0.5, 0.5]), comptador\n",
    "\n",
    "    elif state == \"L\":\n",
    "        comptador = 0 \n",
    "        return np.random.choice([\"L\", \"NC\"], p=[0.4, 0.6]), comptador \n",
    "\n",
    "def choose_action(state):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(actions)\n",
    "    qvals = Q[state]\n",
    "    max_val = max(qvals.values())\n",
    "    best_actions = [a for a in actions if qvals[a] == max_val]\n",
    "    return random.choice(best_actions)\n",
    "\n",
    "state = \"C\"\n",
    "\n",
    "for i in range(n_iter):\n",
    "    action = choose_action(state)\n",
    "    reward = get_reward(state, action)\n",
    "    next_s, comptador = next_state(state, action, comptador)\n",
    "    Q[state][action] += alpha * (reward + gamma * max(Q[next_s].values()) - Q[state][action])\n",
    "    state = next_s\n",
    "\n",
    "print(\"Q-table:\")\n",
    "for s in Q:\n",
    "    print(f\"{s}: {Q[s]}\") \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25272d27-2c6b-4f52-afaf-5b65fa69a510",
   "metadata": {},
   "source": [
    "### Exercici 3: Impacte dels paràmetres d'aprenentatge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "311edf3e-7793-4829-bcc7-a14a8bd641f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fem una funció que cridi els valors dels paràmetres per tal de no possar-ho manualment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "927cdce0-50dd-42bc-89d3-919948886940",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\"C\", \"NC\", \"L\"]\n",
    "actions = [1, 2, 3] \n",
    "\n",
    "def get_reward(state, action):\n",
    "    if state == \"C\":\n",
    "        reward = {1: 6, 2: 3, 3: 0}[action]\n",
    "    elif state == \"NC\":\n",
    "        reward = {1: -3, 2: -1, 3: 1}[action]\n",
    "    elif state == \"L\":\n",
    "        reward = {1: -5, 2: -3, 3: 2}[action]\n",
    "    \n",
    "    if action == 1:\n",
    "        return np.random.choice([reward, -1], p=[0.9, 0.1])\n",
    "    if action == 2:\n",
    "        return np.random.choice([reward, reward - 2], p=[0.8, 0.2])\n",
    "    if action == 3:\n",
    "        return np.random.choice([reward, -1], p=[0.95, 0.05])\n",
    "\n",
    "def next_state(state, action, comptador):\n",
    "\n",
    "    if action in [1,2]:\n",
    "        comptador += 1\n",
    "    \n",
    "    if comptador > 5:\n",
    "        return \"L\", comptador \n",
    "        \n",
    "    if state == \"C\":\n",
    "        return (np.random.choice([\"NC\", \"C\"], p=[0.7, 0.3]), comptador) if action in [1, 2] else (np.random.choice([\"C\", \"NC\"], p=[0.4, 0.6]), comptador)\n",
    "    \n",
    "    elif state == \"NC\":\n",
    "        return np.random.choice([\"C\", \"NC\"], p=[0.5, 0.5]), comptador\n",
    "\n",
    "    elif state == \"L\":\n",
    "        comptador = 0 \n",
    "        return np.random.choice([\"L\", \"NC\"], p=[0.4, 0.6]), comptador        \n",
    "\n",
    "#Fem una funció que executi tot l'algoritme\n",
    "def Q():\n",
    "    #Provarem totes les combinacions de alpha, gamma i epsilon possibles entre 0.1 i 0.9 amb un increment de 0.1, a través de bucles que cridin a aquests valors.\n",
    "    valor= np.arange(0.1, 1.0, 0.1) \n",
    "    \n",
    "    for alpha in valor:\n",
    "        for gamma in valor:\n",
    "            for epsilon in valor:\n",
    "                #Posem el comptador, l'estat pel que comencem i el nombre de iteracions de cada execució dins de la funció\n",
    "                #La inicialització també a dins per tal de que siguin independents les execucions i comencin en 0\n",
    "                Q = {s: {a: 0 for a in actions} for s in states}\n",
    "                comptador = 0\n",
    "                state = \"C\"\n",
    "                n_iter = 100\n",
    "                \n",
    "                def choose_action(state):\n",
    "                    if random.random() < epsilon:\n",
    "                        return random.choice(actions)\n",
    "                    qvals = Q[state]\n",
    "                    max_val = max(qvals.values())\n",
    "                    best_actions = [a for a in actions if qvals[a] == max_val]\n",
    "                    return random.choice(best_actions)\n",
    "                \n",
    "                for i in range(n_iter):\n",
    "                    action = choose_action(state)\n",
    "                    reward = get_reward(state, action)\n",
    "                    next_s, comptador = next_state(state, action, comptador)\n",
    "                    Q[state][action] += alpha * (reward + gamma * max(Q[next_s].values()) - Q[state][action])\n",
    "                    state = next_s\n",
    "\n",
    "                #Per a que mostri els resultats de cada combinació, indicant quins són els paràmetres\n",
    "                print(f\"\\n alpha={alpha}, gamma={gamma}, epsilon={epsilon}\")\n",
    "                for s in Q:\n",
    "                    print(f\"{s}: {Q[s]}\")\n",
    "#Si volem veure les 729 combinacions només s'ha d'escriure Q() i executar\n",
    "#Si volem veure de manera més general, canviem l'increment a 0.3 per tal de veure les 27 combinacions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ff29c7-7196-485a-87f8-5b6268c39618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b75b66-d774-4cc5-8607-2a70f72272d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
